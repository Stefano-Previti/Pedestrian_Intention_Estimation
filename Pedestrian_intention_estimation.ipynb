{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMtHKkaMVgZW7ssHX9wZTPz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Stefano-Previti/Pedestrian_Intention_Estimation/blob/main/Pedestrian_intention_estimation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "▶**LOADING AND PREPARING THE DATASET**"
      ],
      "metadata": {
        "id": "5R1Q24yAAg6g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**⏰Citation**:"
      ],
      "metadata": {
        "id": "mSEzZTQWCI0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bibtex_entries = \"\"\"\n",
        "@inproceedings{rasouli2017they,\n",
        "  title={Are They Going to Cross? A Benchmark Dataset and Baseline for Pedestrian Crosswalk Behavior},\n",
        "  author={Rasouli, Amir and Kotseruba, Iuliia and Tsotsos, John K},\n",
        "  booktitle={ICCVW},\n",
        "  pages={206--213},\n",
        "  year={2017}\n",
        "}\n",
        "\n",
        "@inproceedings{rasouli2018role,\n",
        "  title={It is Not All About Size: On the Role of Data Properties in Pedestrian Detection},\n",
        "  author={Rasouli, Amir and Kotseruba, Iuliia and Tsotsos, John K},a\n",
        "  booktitle={ECCVW},\n",
        "  year={2018}\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "print(bibtex_entries)"
      ],
      "metadata": {
        "id": "91bIDIN9CK4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚓Loading the first 10 video of the JAAD dataset."
      ],
      "metadata": {
        "id": "SuTbowXnCNvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Importing necessary modules\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Defining the path to your files on Google Drive\n",
        "dataset = '/content/drive/My Drive/JAAD_clips'\n",
        "\n",
        "# Creating the directory to extract the contents if it doesn't exist\n",
        "dataset_dir = '/content/data/JAAD_dataset'\n",
        "os.makedirs(dataset_dir, exist_ok=True)\n",
        "\n",
        "# Defining a function to copy the 22 videos\n",
        "def copy_videos(src_dir, dst_dir):\n",
        "\n",
        "    # Listing all files in the source directory\n",
        "    all_files = os.listdir(src_dir)\n",
        "\n",
        "    # Filtering only video files\n",
        "    video_files = [file for file in all_files if file.endswith('.mp4')]\n",
        "\n",
        "    # Sorting files by name\n",
        "    video_files.sort()\n",
        "\n",
        "    # Copying the videos\n",
        "    for video in video_files:\n",
        "        src_file = os.path.join(src_dir, video)\n",
        "        dst_file = os.path.join(dst_dir, video)\n",
        "        print(f\"Copying: {video}\")\n",
        "        shutil.copy(src_file, dst_file)\n",
        "\n",
        "# Copying the videos\n",
        "copy_videos(dataset, dataset_dir)\n"
      ],
      "metadata": {
        "id": "1F9XFGuhCM5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Verification of the extraction by listing files in all subdirectories\n",
        "def verify_extraction(directory, num_files_to_check=5):\n",
        "    # Walking through all directories and files\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        print(f'Checking directory: {root}')\n",
        "        files.sort()\n",
        "        # Showing some files (up to num_files_to_check) in this directory\n",
        "        for i, file_name in enumerate(files[:num_files_to_check]):\n",
        "            file_path = os.path.join(root, file_name)\n",
        "            print(f'File {i+1}: {file_path}')\n",
        "\n",
        "# Running the verification function\n",
        "verify_extraction(dataset_dir)"
      ],
      "metadata": {
        "id": "ub1B6QBzCT27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚓Download of the annotations from the repo https://github.com/ykotseruba/JAAD?tab=readme-ov-file."
      ],
      "metadata": {
        "id": "D2vqTm0eCXNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Cloning the repository from GitHub\n",
        "!git clone https://github.com/ykotseruba/JAAD.git\n",
        "\n",
        "# Compressing the 'annotations' directories into ZIP files\n",
        "import shutil\n",
        "shutil.make_archive('/content/annotations', 'zip', 'JAAD/', 'annotations')\n",
        "\n",
        "# Extracting the ZIP file for Bounding Box and frame annotations\n",
        "with zipfile.ZipFile('annotations.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/annotations')"
      ],
      "metadata": {
        "id": "DllAC-1zCaUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚓Installing MediaPipe for pose estimation beacuse is faster in an enviroment like colab."
      ],
      "metadata": {
        "id": "YWFdCtVlCePn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe"
      ],
      "metadata": {
        "id": "Ei1ikuoyCgWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚓Custom Dataset class for the JAAD dataset. This class extracts local context, 2D location trajectory and pose keypoints from the dataset for pedestrian analysis."
      ],
      "metadata": {
        "id": "WOGPGZVdCi6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Pillow\n",
        "import PIL.Image as Image\n",
        "import cv2\n",
        "import torch\n",
        "import mediapipe as mp\n",
        "import xml.etree.ElementTree as ET\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "class JAADDataset(Dataset):\n",
        "    def __init__(self, data_dir,annotations_dir, image_extension='.png'):\n",
        "        \"\"\"\n",
        "        Initializing the JAADDataset.\n",
        "\n",
        "        Parameters:\n",
        "        - data_dir (str): Directory containing the image data.\n",
        "        - annotations_dir (str): Directory containing annotations XML files.\n",
        "        - image_extension (str): The extension of image files in the dataset (default: '.png').\n",
        "        \"\"\"\n",
        "        self.data_dir = data_dir\n",
        "        self.image_extension = image_extension\n",
        "\n",
        "        # Parsing general annotations\n",
        "        self.annotations = self._parse_annotations(annotations_dir)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieving a data sample at the specified index.\n",
        "\n",
        "        Parameters:\n",
        "        - idx (int): Index of the data sample to retrieve.\n",
        "\n",
        "        Returning:\n",
        "        - dict: A dictionary containing the following keys:\n",
        "            - 'local_context' (Tensor): Tensor of stacked images around the pedestrian.\n",
        "            - 'location_trajectory' (Tensor): 2D coordinates of the pedestrian's trajectory.\n",
        "            - 'pose_keypoints' (Tensor): Keypoints for the pedestrian's pose.\n",
        "            - 'video_id'\n",
        "            -'pedestrian'\n",
        "            -'cross'\n",
        "            -'frame_id'\n",
        "        \"\"\"\n",
        "        annotation= self.annotations[idx]\n",
        "        if annotation is not None:\n",
        "          video_id=annotation['video_id']\n",
        "          cross=annotation['cross']\n",
        "          pedestrian=annotation['pedestrian']\n",
        "          frame_id=annotation['frame_id']\n",
        "\n",
        "\n",
        "\n",
        "        # Loading images and related data\n",
        "        if annotation is not None:\n",
        "          frame = self._load_frame(annotation)\n",
        "          bbox = annotation['bbox']\n",
        "          local_context = self._get_local_context(frame, bbox)\n",
        "          location_trajectory = self._get_location_trajectory(annotation)\n",
        "          pose_keypoints = self._get_pose_keypoints(frame,bbox)\n",
        "\n",
        "        return {\n",
        "              'local_context': torch.tensor(local_context, dtype=torch.float32),\n",
        "              'location_trajectory': torch.tensor(location_trajectory, dtype=torch.float32),\n",
        "              'pose_keypoints': torch.tensor(pose_keypoints, dtype=torch.float32),\n",
        "              'cross':cross,\n",
        "              'pedestrian':pedestrian,\n",
        "              'frame_id':frame_id,\n",
        "              'video_id':video_id,\n",
        "        }\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the length of the general annotations.\n",
        "        \"\"\"\n",
        "        return len(self.annotations)\n",
        "\n",
        "\n",
        "    def _parse_annotations(self, directory, max_files=10):\n",
        "        \"\"\"\n",
        "        Parse annotations from XML files and collect them per video.\n",
        "\n",
        "        Parameters:\n",
        "        - directory (str): Directory containing annotations XML files.\n",
        "        - max_files (int): Maximum number of files to process.\n",
        "\n",
        "        Returns:\n",
        "        - annotations (dict): Dictionary with video keys and lists of annotations.\n",
        "        \"\"\"\n",
        "        general_annotations = []\n",
        "        video_index = 1\n",
        "\n",
        "        for root_dir, dirs, files in os.walk(directory):\n",
        "            files.sort()\n",
        "\n",
        "            for filename in files:\n",
        "                if video_index > max_files:\n",
        "                    break\n",
        "                if filename.endswith('.xml'):\n",
        "                    file_path = os.path.join(root_dir, filename)\n",
        "                    try:\n",
        "                        tree = ET.parse(file_path)\n",
        "                        root = tree.getroot()\n",
        "                        pedestrian_count = 1\n",
        "                        for track in root.findall('track'):\n",
        "                            if track is not None:\n",
        "                                pedestrian=f'pedestrian_{pedestrian_count}'\n",
        "                                pedestrian_count+=1\n",
        "                                boxes = track.findall('box')\n",
        "                                for box in boxes:\n",
        "                                    frame_id = int(box.get('frame'))\n",
        "                                    xbr = float(box.get('xbr'))\n",
        "                                    xtl = float(box.get('xtl'))\n",
        "                                    ybr = float(box.get('ybr'))\n",
        "                                    ytl = float(box.get('ytl'))\n",
        "                                    cross = str(box.get('cross'))\n",
        "                                    if cross == 'crossing':\n",
        "                                        cross = 1\n",
        "                                    else:\n",
        "                                        cross = 0\n",
        "\n",
        "                                    general_annotations.append({\n",
        "                                        'frame_id': frame_id,\n",
        "                                        'bbox': [xbr, xtl, ybr, ytl],\n",
        "                                        'video_id': video_index,\n",
        "                                        'cross': cross,\n",
        "                                        'pedestrian': pedestrian\n",
        "                                    })\n",
        "\n",
        "                        video_index += 1\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing file {file_path}: {e}\")\n",
        "\n",
        "        return general_annotations\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def _load_frame(self, annotation):\n",
        "          \"\"\"\n",
        "          Loads a frame from a video based on the annotation data.\n",
        "\n",
        "          Parameters:\n",
        "          - annotation (dict): Annotation data containing the frame ID and video name.\n",
        "\n",
        "          Returns:\n",
        "          - frame (numpy array): Extracted frame from the video.\n",
        "          \"\"\"\n",
        "          frame_id = annotation['frame_id']\n",
        "          video_id = annotation['video_id']\n",
        "\n",
        "          # Extracting the video name from the annotation\n",
        "          video_name = f\"video_{video_id:04d}.mp4\"\n",
        "          print(f\"Video: {video_name}\")\n",
        "\n",
        "\n",
        "          # Building the path to the video file\n",
        "          video_path = os.path.join(self.data_dir,video_name)\n",
        "\n",
        "          # Opening the video file\n",
        "          cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "          if not cap.isOpened():\n",
        "              raise ValueError(f\"Cannot open the video file: {video_path}\")\n",
        "\n",
        "          # Setting the current frame position\n",
        "          cap.set(cv2.CAP_PROP_POS_FRAMES, frame_id)\n",
        "          ret, frame = cap.read()\n",
        "          cap.release()\n",
        "\n",
        "          if not ret:\n",
        "              raise ValueError(f\"Cannot read frame {frame_id} from video: {video_path}\")\n",
        "\n",
        "          # Converting the image from BGR to RGB\n",
        "          frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "          return frame\n",
        "\n",
        "    def _get_local_context(self, frame, bbox):\n",
        "          \"\"\"\n",
        "          Extracting the local context around the pedestrian by cropping and resizing the frame.\n",
        "\n",
        "          Parameters:\n",
        "          - frame (numpy array): Image frame containing the pedestrian.\n",
        "          - bbox (list): Bounding box coordinates [xtl, ytl, xbr, ybr] around the pedestrian.\n",
        "\n",
        "          Returning:\n",
        "          - numpy array: Cropped and resized frame to [224, 224] for local context analysis using bicubic interpolation.\n",
        "          \"\"\"\n",
        "          # Unpacking the bounding box coordinates\n",
        "          xbr, xtl, ybr , ytl= bbox\n",
        "\n",
        "          # Cropping the region around the pedestrian using the bounding box\n",
        "          cropped_frame = frame[int(ytl):int(ybr), int(xtl):int(xbr)]\n",
        "\n",
        "          # Resizing the cropped frame to the target size [224, 224] using bicubic LANCZOS\n",
        "          new_width = 224\n",
        "          new_height = 224\n",
        "          new_size = (new_width, new_height)\n",
        "\n",
        "          #Converting to PIL Image for resizing\n",
        "          local_context_image = Image.fromarray(cropped_frame)\n",
        "          image = local_context_image.resize(new_size, Image.LANCZOS)\n",
        "          #Converting back to numpy array for MediaPipe\n",
        "          image_np = np.array(image)\n",
        "\n",
        "          return image_np\n",
        "\n",
        "    def _get_location_trajectory(self, annotation):\n",
        "        \"\"\"\n",
        "        Extracting the 2D location trajectory from the annotation data.\n",
        "\n",
        "        Parameters:\n",
        "        - annotation (dict): Annotation data containing bounding boxes.\n",
        "\n",
        "        Returning:\n",
        "        - bbox : bounding box coordinates representing\n",
        "          the pedestrian's trajectory.\n",
        "        \"\"\"\n",
        "        bbox = annotation['bbox']\n",
        "\n",
        "        return bbox\n",
        "\n",
        "\n",
        "    def _get_pose_keypoints(self, frame, bbox):\n",
        "            \"\"\"\n",
        "            Extracting the (x, y) keypoints of the pedestrian's pose from the given image using MediaPipe Pose.\n",
        "            If no pose is detected, returning a tensor of 36 zeros.\n",
        "\n",
        "            Parameters:\n",
        "            - frame (numpy array): Image frame containing the pedestrian.\n",
        "            - bbox (list): Bounding box with coordinates [xtl, ytl, xbr, ybr] around the pedestrian.\n",
        "\n",
        "            Returning:\n",
        "            - Tensor: A tensor of shape (36,) containing the (x, y) coordinates for 18 keypoints or zeros.\n",
        "            \"\"\"\n",
        "\n",
        "            # Initializing the pose detection model\n",
        "            mp_pose = mp.solutions.pose\n",
        "\n",
        "            # Starting with min_detection_confidence of 0.50 and reducing by 0.05 until 0.25\n",
        "            confidence = 0.50\n",
        "            while confidence >= 0.25:\n",
        "                with mp_pose.Pose(static_image_mode=True, min_detection_confidence=confidence) as pose:\n",
        "                    # Processing the image to detect pose landmarks\n",
        "                    image = self._get_local_context(frame, bbox)\n",
        "                    results = pose.process(image)\n",
        "\n",
        "                    # Specifying the indices of 18 specific landmarks to extract\n",
        "                    selected_landmarks = [\n",
        "                        2,  # Left Eye\n",
        "                        5,  # Right Eye\n",
        "                        7,  # Left Ear\n",
        "                        8,  # Right Ear\n",
        "                        11, # Left Shoulder\n",
        "                        12, # Right Shoulder\n",
        "                        13, # Left Elbow\n",
        "                        14, # Right Elbow\n",
        "                        15, # Left Wrist\n",
        "                        16, # Right Wrist\n",
        "                        23, # Left Hip\n",
        "                        24, # Right Hip\n",
        "                        25, # Left Knee\n",
        "                        26, # Right Knee\n",
        "                        27, # Left Ankle\n",
        "                        28, # Right Ankle\n",
        "                        33, # Left Heel\n",
        "                        34  # Right Heel\n",
        "                    ]\n",
        "\n",
        "                    # Extracting the (x, y) coordinates for the selected landmarks\n",
        "                    landmarks_xy = []\n",
        "                    if results.pose_landmarks is not None:  # Checking if landmarks were detected\n",
        "                        for idx in selected_landmarks:\n",
        "                            if idx < len(results.pose_landmarks.landmark):  # Checking if index is within bounds\n",
        "                                landmark = results.pose_landmarks.landmark[idx]\n",
        "                                x = landmark.x * image.shape[1]\n",
        "                                y = landmark.y * image.shape[0]\n",
        "                            else:\n",
        "                                x = 0\n",
        "                                y = 0\n",
        "                            if x<0 or y<0 or x>224 or y>224:\n",
        "                                x=0\n",
        "                                y=0\n",
        "                            landmarks_xy.extend([x, y])\n",
        "                    else:\n",
        "                        # If no landmarks were detected, fill with zeros\n",
        "                        landmarks_xy = [0] * 36\n",
        "\n",
        "                    # Converting the list of coordinates into a PyTorch tensor\n",
        "                    pose_keypoints = torch.tensor(landmarks_xy, dtype=torch.float32)\n",
        "\n",
        "                    # Check if pose_keypoints is not a zero vector\n",
        "                    if torch.sum(pose_keypoints) > 0:\n",
        "                        return pose_keypoints\n",
        "\n",
        "                # Reduce confidence by 0.05\n",
        "                confidence -= 0.05\n",
        "\n",
        "            # If no valid pose_keypoints were found, return the zero vector\n",
        "            return torch.tensor([0] * 36, dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "YoFjN11FCnoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**▶INPUT ACQUISITION**"
      ],
      "metadata": {
        "id": "hWbnwxpUCsUX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚓Acquisition of the input and saving in the drive."
      ],
      "metadata": {
        "id": "1AcpNVrdC2n3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "\n",
        "# Mounting Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path in Google Drive where tensors will be saved\n",
        "input_tensors = '/content/drive/My Drive/pedestrian_input_tensors/'\n",
        "\n",
        "# Ensuring the directory exists\n",
        "os.makedirs(input_tensors, exist_ok=True)\n",
        "\n",
        "# Defining extraction directory\n",
        "annotations_dir = '/content/annotations'\n",
        "\n",
        "# Assuming JAADDataset is a custom dataset class\n",
        "# Creating the custom Dataset\n",
        "JAAD_dataset = JAADDataset(dataset_dir, annotations_dir)\n",
        "\n",
        "# Creating the DataLoader\n",
        "batch_size = 2\n",
        "data_loader = DataLoader(JAAD_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "def extract_and_save(data_loader, input_tensors_dir):\n",
        "    # Creating the save directory if it doesn't exist\n",
        "    os.makedirs(input_tensors_dir, exist_ok=True)\n",
        "    sample_count = 0\n",
        "\n",
        "    # Iterating over each batch from the DataLoader\n",
        "    for batch_idx, batch in enumerate(data_loader):\n",
        "        video_ids = batch['video_id']\n",
        "        frame_ids = batch['frame_id']\n",
        "        location_trajectories = batch['location_trajectory']\n",
        "        pose_keypoints = batch['pose_keypoints']\n",
        "        local_context = batch['local_context']\n",
        "        pedestrians = batch['pedestrian']\n",
        "        cross = batch['cross']\n",
        "\n",
        "        # Iterating over each element in the batch\n",
        "        for i in range(len(video_ids)):\n",
        "            # Creating a new sample\n",
        "            sample = {\n",
        "                'video_id': video_ids[i],\n",
        "                'frame_id': frame_ids[i],\n",
        "                'location_trajectory': location_trajectories[i],\n",
        "                'pose_keypoints': pose_keypoints[i],\n",
        "                'local_context': local_context[i],\n",
        "                'pedestrian': pedestrians[i],\n",
        "                'cross': cross[i]\n",
        "            }\n",
        "\n",
        "            # Saving the sample\n",
        "            torch.save(sample, os.path.join(input_tensors_dir, f'sample_{sample_count}.pt'))\n",
        "            sample_count += 1\n",
        "\n",
        "    print(f\"Saved {sample_count} samples to {input_tensors_dir}\")\n",
        "\n",
        "# Extract and save\n",
        "extract_and_save(data_loader, input_tensors)\n"
      ],
      "metadata": {
        "id": "ro6HglQPC51S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}